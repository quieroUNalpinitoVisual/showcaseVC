[{"id":0,"href":"/showcasevc/docs/shortcodes/Antialiasing/","title":"Antialiasing","section":"Shortcodes","content":" Antialiasing # Se desea trabajar en una implementacion del algoritmo de antialiasing y estudiar sus fenomenos.\n"},{"id":1,"href":"/showcasevc/docs/shortcodes/Antialiasing/AntialiasingLab/","title":"Antialiasing Lab","section":"Antialiasing","content":" Masking Lab # Seccion de pruebas de antialiasing.\nFlecha derecha: Generar nuevo triangulo\nFlecha izquierda: Pasar antialiasing\n"},{"id":2,"href":"/showcasevc/docs/shortcodes/Antialiasing/AntialiasingRes/","title":"Antialiasing Res","section":"Antialiasing","content":" Academic report # Introducción: # El Antialiasing (suavizado, antiescalonamiento o suavizado de bordes) son un conjunto de tecnicas diferentes las cuales quieren evitar el solapamiento de los bordes debida a representar una señal de alta resolución en un monitor de baja resolución.\nLiteratura: # Métodos de AA: # Para lograr el efecto del antialiasing se realizan varios métodos con distintas aproximaciones:\nSuper sampling:\nBasicamente consiste en realizar un análogo de el muestreo de una señal analoga y aumentar el muestreo de la misma para obtener mejores datos. Multi-sampling:\nSe parece al algoritmo de super sampling pero difiere con el al momento de calculo de cada pixel, donde en SSAA se realizan varios calculos por pixel, en MSAA se realiza solo un calculo por pixel.\nEdge AA:\nEs un método propuesto por Deferred Shading en STALKER. En este se realiza primero un calculo de bordes de una imagen y a partir de esto se usa como máscara para saber en que lugares se debe realizar el algoritmo de AA. AA Morfológico:\nEn este método de dividen las areas de pixeles con colores similares en 3 tipos: Z, L y U. Tanto Z como U se pueden componer de tipos L. Ya con las areas de tipo L distinguidas usa modelado de triangulos para realizar el calculo del antialiasing.\nDeep Learning Super Sample:\nEste algortimo desarrollado por Nvidia hace uso de los subprocesadores de tipo Tensor en las nuevas graficas de la compañia para implementar Super Sampling mediante una IA. AMD FidelityFX Contrast Adaptive Sharpening:\nEste algoritmo de antialiasing temporal es desarrollado por AMD technologies y posee una base de codigo abierto para su uso. Ademas de proveer antialiasing, tambien posee funciones de escalamiento y sharpening.\nImplementación # En el apartado de AntialiasingLab se puede encontrar un ejemplo de implementacion de SSAA basado en la facilidad que tenemos de obtener la referencia real de la figura que estamos modelando, en este caso es un triángulo aleatorio que se dibuja en una cuadrilla.\nEn la funcion que describimos a continuacion en el expandible encontramos los loops que recorren la simulacion de una imagen de mas alta resolución (64 a 1) y van hayando donde el triangulo discreto (en este caso dibujado con la funcion de Quadrille.js) y el triangulo real que se dibuja con la primitiva triangle() de p5. Damos un peso relativo de acuerdo a cuantos pixeles de la imagen de alta resolucion poseen el triangulo y lo dibujamos luego en la cuadrilla de menor resolución\nExpand ↕ p5 code # function supersamp(){ let total = 64; for (let i = 0; i \u0026lt; QUADDIM ; i ++) { for (let j = 0; j \u0026lt; QUADDIM ; j ++) { let count = 0; let si = LENGTH * i +1; let sj = LENGTH * j + 1; for (; si \u0026lt; LENGTH * i + LENGTH - 1 ; si ++) { for (; sj \u0026lt; LENGTH * j + LENGTH - 1 ; sj ++) { if(get(si,sj)[0] != 0){ count += 1; } } } let shade = (count/total)*10; board._memory2D[j][i]=[red(0), green(255*shade), blue(0), alpha(255)]; } } } Resultados # Pre AA\nPost AA\nConclusiones # El algoritmo es capaz de ser paralelizable para un mejor rendimiento. Dada la base tan robusta y sencilla del SS este ha derivado en otros métodos como el DLSS. La información inicial deberia ser capaz de obtenerse sin bordes para dibujarlos con AA (QuadrilleJS en los bordes \u0026ldquo;ïnferiores\u0026rdquo;). Junto con el calculo de bordes daria un muy buen resultado con el algoritmo de Edge AA. Entre mucho mayor sea la resolucion del super sampling, se obtienen profundidades de colores intermedias mas precisas, pero llega un punto en que no se notara la diferencia. Referencias # Algoritmos de antialiasing - Gabriel Mañana Guichón Nvidia DLSS AMD FidelityFx CAS Mas algoritmos de Antialiasing QuadrilleJS\n"},{"id":3,"href":"/showcasevc/docs/shortcodes/Interactions-3D/","title":"Interactions 3 D","section":"Shortcodes","content":" Investigación sobre interacciónes de usuario en 3D # (A Survey of Interaction Techniques for Interactive 3D Environments) - Jacek Jankowski, Martin Hachet\nAcademic report # Introducción: # Teniendo en cuenta la tematica adelantada en clase, y su amplio espectro, decidimos enfocarnos en las interacciones de usuario en 3D. Mas especificamente las interacciones hombre maquina dentro de un espacio virtual simulado en la computadora. Y es que a pesar de que todos hemos crecido y vivido en un entorno 3D y por tanto hemos desarrollado habilidades para interactuar en el mismo, a menudo nos resulta difícil interacturar en entronos 3D interactivos, y segun Jankowski[1] frecuentemente esto es resultado de una interfaz de usuario mal diseñada o debido a una selección inadecuada de técnicas de interacción.\nLiteratura: # Entornos 3D: # Se definen los entornos 3D interactivos como representaciones informáticas del mundo real o espacios imaginarios a través de los cuales los usuarios pueden navegar y en los que pueden interacturar con objetos en tiempo real. A diferencia de los entornos virtuales inmersivos , que utilizan pantallas especializadas y dispositivos de interacción, como pantallas montadas en la cabeza, para crear una sensación de presencia para los usuarios dentro del mundo virtual, los entornos 3D interactivos no se limitan al contexto inmersivo y aprovechan los recursos comunes y general de hardware, es decir mouse y teclado como multi_touch input.\nTendremos en cuenta la clasificación de las interacciones en un ambiente interactivo 3D que encontramos en el libro de Jankowski[1] que habla de 3 tareas de interaccion universales: Navegación, Selección y manipulación, y Control de sistema.\nNavegación: # Se refiere a la tarea motora de mover el punto de vista del usuario a través de un entorno; incluye un componente cognitivo: orientación.\nLos entornos 3D interactivos suelen representar más espacio del que se puede ver desde un solo punto. Los usuarios deben poder moverse dentro del entorno para obtener diferentes vistas de la escena. Este proceso de desplazarse por un entorno virtual mientras se realiza un seguimiento del propio paradero y los objetivos de la tarea es el tema de una serie de técnicas a las que a menudo se hace referencia con nombres como orientación, locomoción, navegación y cámara.\nHay que resaltar que es difícil desarrollar una técnica eficaz para la navegación en entornos 3D interactivos. En primer lugar, el control del punto de vista implica seis grados de libertad : tres dimensiones para la ubicación posicional (traslación) y tres para la ubicación angular (rotación).\nSegun algunas literaturas podemos agrupoar 4 tipos de movimiento de punto de vista para espacios de trabajo 3D interactivos:\nMovimiento general: Movimiento exploratorio, como caminar a través de una simulación de un diseño arquitectónico\nMovimiento dirigido: Movimiento con respecto a un objetivo específico, como moverse para examinar un detalle de un modelo de ingeniería.\nMovimiento de coordenadas especificado: Movimiento a una posición y orientación precisas, como una posición de visualización específica en relación con un modelo CAD: el usuario debe proporcionar la posición y orientación exactas de su destino.\nMovimiento de trayectoria especificado: Movimiento a lo largo de una trayectoria de posición y orientación, como el movimiento de una cámara cinematográfica (corresponde al objetivo Maniobra de Bowman et al. y al objetivo Inspección de Tan et al.).\nSelección y manipulación: # Otra tarea típica que se realiza en un entorno virtual 3D es la selección de objetos y su manipulación directa mediante la especificación su posición, orientación y escala. Encontramos que las técnicas de interacción mas comunes en la literatura para la manipulación 3D incluyen tres tareas fundamentales: traducción de objetos (posicionamiento), rotación de objetos y escalado de objetos. Las técnicas de interacción para la manipulación 3D incluyen tres tareas fundamentales: traducción de objetos (posicionamiento), rotación de objetos y escalado de objetos.\nSeleccion de objetos: # La forma más común de seleccionar objetos en los VE de escritorio es colocar el cursor del mouse sobre un objeto dado y hacer clic en un botón del mouse. La técnica se basa en la proyección de rayos (ray casting); utiliza el rayo desde el punto del ojo a través del píxel actualmente seleccionado por el puntero del mouse para encontrar el primer punto de intersección con la escena (la superficie del objetivo o su superficie aproximada, por ejemplo, el cuadro delimitador). Si el rayo virtual se cruza con un objeto, se puede seleccionar para una mayor manipulación.\nManipulación de objeto: # La manipulación de objetos es otra tarea básica que se realiza en un entorno virtual 3D, especialmente durante la construcción de escenas. Al diseñar una escena con múltiples objetos, los usuarios tienen que realinear y ajustar varias partes repetidamente. Es importante proporcionar una técnica de manipulación de objetos eficiente.\nControl del sistema: # Se refiere a la comunicación entre el usuario y el sistema que no forma parte del entorno virtual.\nSe puede enfocar en una tarea en la que se aplica un comando para cambiar el estado del sistema o el modo de interacción .Se señala que aunque el movimiento del punto de vista y la selección/manipulación se han estudiado ampliamente, se ha investigado muy poco sobre las tareas de control del sistema. Sin embargo, las técnicas de control de aplicaciones se han estudiado intensamente durante los últimos 40 años en las interfaces gráficas de usuario WIMP 2D \u0026ldquo;apuntar y hacer clic\u0026rdquo; (interfaces basadas en ventanas, íconos, menús y un dispositivo señalador, generalmente un mouse).\n# Metodos # "},{"id":4,"href":"/showcasevc/docs/shortcodes/masking/","title":"Masking","section":"Shortcodes","content":" Masking # En esta temática se desarrolló un entorno de ejecución para visualizar los efectos de algunos kernels en matrices de convolucion sobre imagenes para su posterior uso como base en un algoritmo que calcula los bordes de las mismas imágenes mediante derivadas parciales en dos variables.\n"},{"id":5,"href":"/showcasevc/docs/shortcodes/masking/maskingLab/","title":"Masking Lab","section":"Masking","content":" Masking Lab # Seccion de pruebas de matrices de convolucion como paso intermedio para el cálculo de bordes mediante derivadas parciales.\n"},{"id":6,"href":"/showcasevc/docs/shortcodes/masking/maskingRes/","title":"Masking Res","section":"Masking","content":" Masking Results # Conceptos # Lo primero que se realizó fue programar la función que permitirá la convolución de un kernel sobre una imagen de acuerdo con lo encontrado en wikipedia\nExpand ↕ Wiki pseudocode # for each image row in input image: for each pixel in image row: set accumulator to zero for each kernel row in kernel: for each element in kernel row: if element position corresponding* to pixel position then multiply element value corresponding* to pixel value add result to accumulator endif set output image pixel to accumulator la implementación concreta es la siguiente\nExpand ↕ p5 code # p.convolute = function (matrix){ img.loadPixels(); let rtotal = 0.0; let gtotal = 0.0; let btotal = 0.0; let postconv = p.createImage(400,400); for (let i = 1; i \u0026lt; 399; i++){ for (let j = 1; j \u0026lt; 399; j++){ rtotal = 0.0; gtotal = 0.0; btotal = 0.0; for (let k = -1; k \u0026lt;= 1; k++){ for (let l = -1; l \u0026lt;= 1; l++){ let xpos = i+k; let ypos = j+l; let red = p.red(img.get(xpos,ypos)); let green = p.green(img.get(xpos,ypos)); let blue = p.blue(img.get(xpos,ypos)); rtotal += red * matrix[k+1][l+1]; gtotal += green * matrix[k+1][l+1]; btotal += blue * matrix[k+1][l+1]; } } rtotal = p.constrain(rtotal, 0, 255); gtotal = p.constrain(gtotal, 0, 255); btotal = p.constrain(btotal, 0, 255); let c = p.color(rtotal,gtotal,btotal); postconv.set(i,j,c); } } postconv.updatePixels(); img = postconv; img.updatePixels(); p.square(0,200,400); p.drawimg(img,0,200); console.log('convolucion realizada! :D'); } Una derivada parcial nos indica como se mueve la pendiente en ese punto para la funcion (imagen)\nhttps://mathinsight.org/partial_derivative_limit_definition\ncodigo de detección de bordes:\nExpand ↕ p5 code # p.calcborders = function(){ let prebord = p.createImage(400,400); for (let i = 0; i \u0026lt; 400; i++) { for (let j = 0; j \u0026lt; 400; j++) { let g = (p.red(img.get(i,j))+p.green(img.get(i,j))+p.blue(img.get(i,j)))/3; prebord.set(i,j,g); } } let med = [0,0,0,0,0,0,0,0,0]; for (let r = 0; r \u0026lt; 3; r++) { for (let i = 1; i \u0026lt; 400-1; i++) { for (let j = 1; j \u0026lt; 400-1; j++) { med[0] = prebord.get(i - 1,j - 1); med[1] = prebord.get(i,j - 1); med[2] = prebord.get(i + 1,j - 1); med[3] = prebord.get(i - 1,j); med[4] = prebord.get(i,j); med[5] = prebord.get(i + 1,j); med[6] = prebord.get(i - 1,j + 1); med[7] = prebord.get(i,j + 1); med[8] = prebord.get(i + 1,j + 1); med.sort(); prebord.set(i,j,med[4]); } } } let gradx = p.zeroes([400,400]); let grady = p.zeroes([400,400]); for (let i = 0; i \u0026lt; 400-1; i++) { for (let j = 0; j \u0026lt; 400-1; j++) { gradx[i][j] = p.red(img.get(i+1,j)) - p.red(img.get(i,j)); grady[i][j] = p.red(img.get(i,j+1)) - p.red(img.get(i,j)); } } let gradmag = p.zeroes([400,400]); let gradang = p.zeroes([400,400]); for (let i = 0; i \u0026lt; 400-1; i++) { for (let j = 0; j \u0026lt; 400-1; j++) { gradmag[i][j]= p.abs(p.sqrt((gradx[i][j]*gradx[i][j])+(grady[i][j]*grady[i][j]))); gradang[i][j]= p.atan2(grady[i][j],gradx[i][j]); } } let borders = p.zeroes([400,400]); for (let i = 1; i \u0026lt; 400-1; i++) { for (let j = 1; j \u0026lt; 400-1; j++) { if (gradang[i][j] \u0026gt; 0 \u0026amp;\u0026amp; gradang[i + 1][j] \u0026lt; 0) { borders[i][j] = 1; } else if (gradang[i][j] \u0026lt; 0 \u0026amp;\u0026amp; gradang[i + 1][j] \u0026gt; 0) { borders[i][j] = 1; } else if (gradang[i][j] \u0026gt; 0 \u0026amp;\u0026amp; gradang[i][j + 1] \u0026lt; 0) { borders[i][j] = 1; } else if (gradang[i][j] \u0026lt; 0 \u0026amp;\u0026amp; gradang[i][j + 1] \u0026gt; 0) { borders[i][j] = 1; } } } for (let i = 1; i \u0026lt; 400-1; i++) { for (let j = 1; j \u0026lt; 400-1; j++) { if( borders[i][j-1] == 0 \u0026amp;\u0026amp; borders[i-1][j] == 0 \u0026amp;\u0026amp; borders[i+1][j] == 0 \u0026amp;\u0026amp; borders[i][j+1] == 0 ){ borders[i][j] = 0; } } } let postbord = p.createImage(400,400); for (let i = 0; i \u0026lt; 400; i++) { for (let j = 0; j \u0026lt; 400; j++) { if (borders[i][j] == 1) { //postbord.set(i, j, p.color(0,0,0)); postbord.set(i, j, p.color(255,255,255)); } else { //postbord.set(i, j, p.color(255,255,255)); postbord.set(i, j, p.color(0,0,0)); } } } postbord.updatePixels(); console.log(prebord); console.log(postbord); bord = postbord; p.image(bord, 0, 650,400,400); } Imagenes convolucionadas y bordes # Los siguientes resultados poseen un error en la funcion de convolucion que intercambia el valor de verdes por azules y viceversa, visible en el resultado de la convolución, pero esto mismo debido a que es una permutación no afecta el calculo de bordes el cual se hace a partir de la imagen en escala de grises.\nimagen base: # identidad / sin mascara : # sharpen : # ridge : # box blur: # composition: # "},{"id":7,"href":"/showcasevc/docs/shortcodes/members/","title":"Members","section":"Shortcodes","content":" Members # Nicholson Stive Ochoa # Estudiante de Ingenieria de Sistemas y Computacion en la Universidad Nacional de Colombia.\nIntereses 🤔 # Académicos 📘 Desarrollo web Inteligencia artificial Desarrollo en multiplataforma Pasatiempos 🎮🎸 Ver anime y series Hacer ejercicio Daniel Arturo Moreno # Estudiante de Ingenieria de Sistemas y Computacion en la Universidad Nacional de Colombia.\nIntereses 🤔 # Académicos 📘\nDesarrollo Web Arduino Linux Procesamiento de imágenes Programación Competitiva y Algoritmia Pasatiempos 🎮🎸\nGeoguessr RPGs Música Johan Sebastian Romero # Estudiante de Ingenieria de Sistemas y Computacion en la Universidad Nacional de Colombia.\nIntereses 🤔 # Académicos 📘\nDesarrollo Web Procesamiento de imagenes Arquitectura de software Pasatiempos 🎮🎸\nVideojuegos (Wow) Motociclismo off road Anime "},{"id":8,"href":"/showcasevc/docs/shortcodes/shaders/","title":"Shaders","section":"Shortcodes","content":" Shaders # Introducción: # Como etapa final de la materia dimos un vistazo al tema de shaders y su aplicación en distintos ejemplos expuestos por el profesor, como lo son Coloring, Texturing, Image Processing, y Procedural Texturing. Por esta razón, para esta entrega final profundizamos en cada uno de estos temas e hicimos nuestras propias modificaciones y propias implementaciones para entender y mostrar que nuevas cosas se pueden hacer con shaders. shaders Adicionalmente aplicamos estos conocimientos adquiridos a una entrega anterior en la que se aplicaron tecnicas de antialising, obteniendo unos resultados interesantes.\nRevisión de literatura/Antecedentes: # Shader (Sombreador): # Es un programa que ejecuta o realiza cálculos matematicos para manipular los atributos de una o varias primitivas gráficas. Es una tencnología que ha experimentado una rápida evolución destinada a proporcionar al programador una interacción con la unidad de procesamiento gráfico (GPU) hasta ahora imposible. Los sombreadores son utilizados para realizar transformaciones de vértices o coloreado de píxeles, entre otras labores, con el propósito de crear efectos especiales, como iluminación, fuego o niebla.\nLas unidades que ejecutan los programas shaders están en todas las GPU y a día de hoy hay decenas de estas en cada GPU, incluyendo las de gama baja. Desde el momento en que para un procesador un píxel, un triángulo, un vértice o el mapa de una proteína no son más que datos, las unidades que se utilizan para ejecutar estos programas son iguales independientemente del tipo de shader.\nVertex Shader: # En español Sombreador de vértices, es una herramienta capaz de trabajar con la estructura de vértices de figuras modeladas en 3D, y realizar operaciones matemáticas sobre ella para definir colores, texturas e incidencia de la luz.\nFragment Shader: # Los Fragment Shaders son aquellos que procesan un fragmento (por ejemplo, un píxel) generado por la Rasterización como un conjunto de colores y un valor de profundidad. Por lo tanto, estos son conjuntos de instrucciones que deben actuar al mismo tiempo por cada uno de los píxeles de la pantalla y se comportan diferente dependiendo de su posición en la pantalla.\nUna diferencia principal es que un sombreador de vértices puede manipular los atributos de los vértices los cuáles son los puntos de las esquinas de tus polígonos.\nEl sombreador de fragmentos, por otro lado, se encarga de cómo se ven los píxeles entre los vértices. Se interpolan entre los vértices definidos siguiendo reglas específicas.\nReferencias 📚 # [1] The Book of Shaders. Vivo, Patricio \u0026amp; Lowe, Jen\n[2] Shader Basics, Blending \u0026amp; Textures • Shaders for Game Devs (Part 1). Holmér Freya\n[3] Fragment Shader. OpenGL Wiki\n[4] Hard Zone\n[5] Vertex Shader\n"},{"id":9,"href":"/showcasevc/docs/shortcodes/shaders/EdgeAntiAliasing/","title":"Edge Anti Aliasing","section":"Shaders","content":" Edge Anti Aliasing # Seccion de pruebas de el algoritmo propuesto para realizar Edge AA con shaders. Esta sección es una continuación de anteriores entregas en las cuales ya habiamos tratado el tema de mascaras de convolución mediante software y sus efectos en la detección de bordes ademas de nuestra entrega acerca de los algoritmos de Super Sampling.\nPruebas del Algoritmo # Controles 🕹️ # Haz click y presiona una de las siguientes teclas:\n⬅️ Aplicar Máscara de Ridge ➡️ Edge Anti Aliasing Implementación # Para logar implementar lo que haria el algoritmo de Edge AA hicimos uso de dos conjuntos de shaders distintos, uno para poder utilizar la mascara de convolucion correspondiente a Ridge para guardar en un archivo que despues usariamos como insumo para nuestro shader custom basado en mascaras que realiza el suavizado de las texturas mediante promedios de colores en un area de 3x3 circundante al borde solo en los lugares donde el color de la mascara de ridge es muy marcada ( con un valor de 0.2 en todas las componentes de color).\nedgeaa.js # Expand ↕ let baseimg; let edgemask; let antialias; let edgeimg; let edged = false; function preload(){ edgemask = readShader(\u0026quot;/showcasevc/p5files/edgeaa/edgemask.frag\u0026quot;,{varyings: Tree.texcoords}); antialias = readShader(\u0026quot;/showcasevc/p5files/edgeaa/antialias.frag\u0026quot;,{varyings: Tree.texcoords}); baseimg = loadImage('/showcasevc/sketches/mayonesito.jpg'); edgeimg = loadImage('/showcasevc/sketches/bordesmayonesito.png'); } function setup() { createCanvas(500, 500, WEBGL); noStroke(); noLoop(); textureMode(NORMAL); loadPixels(); } function draw() { background(0); image(baseimg,-250,-250); } function keyPressed () { if(keyCode === RIGHT_ARROW){ applyAA(); } if(keyCode === LEFT_ARROW){ applyMask(); } if(keyCode === UP_ARROW){ resetImg(); } } function resetImg(){ edged = false; edgemask.setUniform('dosom',false); quad(-250,-250,250,-250,250,250,-250,250); } function applyAA(){ if(edged){ shader(antialias); antialias.setUniform('textureimg',baseimg); antialias.setUniform('textureedge',edgeimg); antialias.setUniform('texOffset',[1/500,1/500]); quad(-250,-250,250,-250,250,250,-250,250); } } function applyMask(){ background(0); shader(edgemask); edgemask.setUniform('dosom',true); edgemask.setUniform('texture',baseimg); edgemask.setUniform('texOffset',[1/500,1/500]); edgemask.setUniform('mask',[-1,-1,-1,-1,8,-1,-1,-1,-1]); quad(-250,-250,250,-250,250,250,-250,250); edged = true; } edgemask.frag # Expand ↕ precision mediump float; uniform sampler2D texture; uniform vec2 texOffset; // holds the 3x3 kernel uniform float mask[9]; uniform bool dosom; varying vec2 texcoords2; void main() { // 1. Use offset to move along texture space. // In this case to find the texcoords of the texel neighbours. vec2 tc0 = texcoords2 + vec2(-texOffset.s, -texOffset.t); vec2 tc1 = texcoords2 + vec2( 0.0, -texOffset.t); vec2 tc2 = texcoords2 + vec2(+texOffset.s, -texOffset.t); vec2 tc3 = texcoords2 + vec2(-texOffset.s, 0.0); // origin (current fragment texcoords) vec2 tc4 = texcoords2 + vec2( 0.0, 0.0); vec2 tc5 = texcoords2 + vec2(+texOffset.s, 0.0); vec2 tc6 = texcoords2 + vec2(-texOffset.s, +texOffset.t); vec2 tc7 = texcoords2 + vec2( 0.0, +texOffset.t); vec2 tc8 = texcoords2 + vec2(+texOffset.s, +texOffset.t); // 2. Sample texel neighbours within the rgba array vec4 rgba[9]; rgba[0] = texture2D(texture, tc0); rgba[1] = texture2D(texture, tc1); rgba[2] = texture2D(texture, tc2); rgba[3] = texture2D(texture, tc3); rgba[4] = texture2D(texture, tc4); rgba[5] = texture2D(texture, tc5); rgba[6] = texture2D(texture, tc6); rgba[7] = texture2D(texture, tc7); rgba[8] = texture2D(texture, tc8); // 3. Apply convolution kernel vec4 convolution; for (int i = 0; i \u0026lt; 9; i++) { convolution += rgba[i]*mask[i]; } // 4. Set color from convolution gl_FragColor = dosom ? vec4(convolution.rgb, 1.0) : rgba[4]; } antialias.frag # Expand ↕ precision mediump float; uniform sampler2D textureimg; uniform sampler2D textureedge; uniform vec2 texOffset; // we need our interpolated tex coord varying vec2 texcoords2; void main() { vec4 edgepoint = texture2D(textureedge, texcoords2 + vec2(0.0)); vec2 tc0 = texcoords2 + vec2(-texOffset.s, -texOffset.t); vec2 tc1 = texcoords2 + vec2( 0.0, -texOffset.t); vec2 tc2 = texcoords2 + vec2(+texOffset.s, -texOffset.t); vec2 tc3 = texcoords2 + vec2(-texOffset.s, 0.0); vec2 tc4 = texcoords2 + vec2( 0.0, 0.0); vec2 tc5 = texcoords2 + vec2(+texOffset.s, 0.0); vec2 tc6 = texcoords2 + vec2(-texOffset.s, +texOffset.t); vec2 tc7 = texcoords2 + vec2( 0.0, +texOffset.t); vec2 tc8 = texcoords2 + vec2(+texOffset.s, +texOffset.t); vec4 rgba[9]; rgba[0] = texture2D(textureimg, tc0); rgba[1] = texture2D(textureimg, tc1); rgba[2] = texture2D(textureimg, tc2); rgba[3] = texture2D(textureimg, tc3); rgba[4] = texture2D(textureimg, tc4); rgba[5] = texture2D(textureimg, tc5); rgba[6] = texture2D(textureimg, tc6); rgba[7] = texture2D(textureimg, tc7); rgba[8] = texture2D(textureimg, tc8); vec4 sum = vec4(0.0); for (int i = 0; i \u0026lt; 9; i++) { sum += rgba[i]; } if( (edgepoint.r \u0026gt; 0.2 ) \u0026amp;\u0026amp; (edgepoint.g \u0026gt; 0.2 ) \u0026amp;\u0026amp; (edgepoint.b \u0026gt; 0.2 )){ gl_FragColor = sum*vec4(1.0/9.0); }else{ gl_FragColor = texture2D(textureimg, texcoords2 + vec2(0.0,0.0)); } } Resultados # Como podemos ver en la anterior imagen los colores presentes bajo el ojo pero sobre el pico del pato de la imagen estan intactos, donde no se presentaban bordes. Pero en una zona mas rica en bordes como lo es a la izquierda del ojo y en el ojo mismo nuestro algoritmo realizo el trabajo de suavizado perfectamente para evitar los escalones en cada pixel y tener una imagen menos agreste en los bordes.\nConclusiones # El algoritmo de EdgeAA nos permite mas que una mejor aplicacion del algoritmo de Super Sampling, una versión mas eficiente de esta misma. Esto debido a que la variación que realiza respecto a otros métodos es la no aplicación de forma general a la imagen sino en áreas especificas y con un énfasis que se puede acomodar por el usuario o parametrizador el valor de corte de 0.2 en las componentes de color de la imagen de bordes nos permitio una aplicacion aceptable en la imagen sin llegar a los artefactos que se llegan a presentar si realizaramos un corte en 0.5 o en 0.1 por el sobreuso o el poco uso del suavizado. "},{"id":10,"href":"/showcasevc/docs/shortcodes/shaders/imageProcessing/","title":"Image Processing","section":"Shaders","content":" Image Processing # En esta sección tambien vamos a tener como referencia nuestra primera entrega en la sección de masking para afianzar los conceptos y comparar el rendimiento.\nDesarrollo de los Ejercicios # Controles # Haz click y presiona una de las siguientes teclas:\n⬅️ Resetear ➡️ Aplicar Máscara de Ridge Imagen # Video # (puede tardar en cargar unos segundos) Implementación # Imagen # Expand ↕ let baseimg; let rstimg; let maskShader; function preload(){ maskShader = readShader(\u0026quot;/showcasevc/p5files/shadersJohan/mask.frag\u0026quot;,{varyings: Tree.texcoords}); baseimg = loadImage('/showcasevc/sketches/mayonesito.jpg'); rstimg = baseimg; } function setup() { createCanvas(500, 500, WEBGL); noStroke(); noLoop(); textureMode(NORMAL); loadPixels(); } function draw() { background(0); image(rstimg,-250,-250); shader(maskShader); maskShader.setUniform('texture',baseimg); maskShader.setUniform('texOffset',[1/500,1/500]); maskShader.setUniform('mask',[-1,-1,-1,-1,8,-1,-1,-1,-1]); maskShader.setUniform('dosom',false); } function keyPressed () { if(keyCode === RIGHT_ARROW){ applyMask(); } if(keyCode === LEFT_ARROW){ resetMask(); } } function resetMask(){ maskShader.setUniform('dosom',false); quad(-250,-250,250,-250,250,250,-250,250); } function applyMask(){ maskShader.setUniform('dosom',true); quad(-250,-250,250,-250,250,250,-250,250); } Video # Expand ↕ let basevid; let maskShader; let isVidReady = false; let dosom = false; function onVideoLoad(){ isVidReady = true; } function preload(){ maskShader = readShader(\u0026quot;/showcasevc/p5files/shadersJohan/mask.frag\u0026quot;,{varyings: Tree.texcoords}); basevid = createVideo('/showcasevc/sketches/fingers.webm',onVideoLoad()); basevid.hide(); } function setup() { createCanvas(300, 300, WEBGL); basevid.size(300,300); noStroke(); } function draw() { background(0); if(isVidReady){ quad(-150,-150,150,-150,150,150,-150,150); } basevid.loop(); shader(maskShader); maskShader.setUniform('texture',basevid); maskShader.setUniform('texOffset',[1/300,1/300]); maskShader.setUniform('mask',[-1,-1,-1,-1,8,-1,-1,-1,-1]); maskShader.setUniform('dosom',dosom); } function keyPressed () { if(keyCode === RIGHT_ARROW){ applyMask(); } if(keyCode === LEFT_ARROW){ resetMask(); } } function resetMask(){ dosom = false; maskShader.setUniform('dosom',dosom); if(isVidReady){ quad(-150,-150,150,-150,150,150,-150,150); } } function applyMask(){ dosom = true; maskShader.setUniform('dosom',dosom); if(isVidReady){ quad(-150,-150,150,-150,150,150,-150,150); } } mask.frag # Expand ↕ precision mediump float; uniform sampler2D texture; uniform vec2 texOffset; // holds the 3x3 kernel uniform float mask[9]; uniform bool dosom; // we need our interpolated tex coord varying vec2 texcoords2; void main() { // 1. Use offset to move along texture space. // In this case to find the texcoords of the texel neighbours. vec2 tc0 = texcoords2 + vec2(-texOffset.s, -texOffset.t); vec2 tc1 = texcoords2 + vec2( 0.0, -texOffset.t); vec2 tc2 = texcoords2 + vec2(+texOffset.s, -texOffset.t); vec2 tc3 = texcoords2 + vec2(-texOffset.s, 0.0); // origin (current fragment texcoords) vec2 tc4 = texcoords2 + vec2( 0.0, 0.0); vec2 tc5 = texcoords2 + vec2(+texOffset.s, 0.0); vec2 tc6 = texcoords2 + vec2(-texOffset.s, +texOffset.t); vec2 tc7 = texcoords2 + vec2( 0.0, +texOffset.t); vec2 tc8 = texcoords2 + vec2(+texOffset.s, +texOffset.t); // 2. Sample texel neighbours within the rgba array vec4 rgba[9]; rgba[0] = texture2D(texture, tc0); rgba[1] = texture2D(texture, tc1); rgba[2] = texture2D(texture, tc2); rgba[3] = texture2D(texture, tc3); rgba[4] = texture2D(texture, tc4); rgba[5] = texture2D(texture, tc5); rgba[6] = texture2D(texture, tc6); rgba[7] = texture2D(texture, tc7); rgba[8] = texture2D(texture, tc8); // 3. Apply convolution kernel vec4 convolution; for (int i = 0; i \u0026lt; 9; i++) { convolution += rgba[i]*mask[i]; } // 4. Set color from convolution gl_FragColor = dosom ? vec4(convolution.rgb, 1.0) : rgba[4]; } Conclusiones # La aplicacion de máscaras mediante el uso de shaders es notablemente más rapido gracias al uso de la tarjeta gráfica del computador. Esto se puede hacer tangible no solo en la velocidad del proceso, sino que tambien en el hecho de que podemos implementar el mismo shader para imagenes en video y poder realizar el proceso en tiempo real en lugar de tener que esperar aproximadamente 20 segundos +/- 1, a poder realizarlo varias veces por segundo. La cantidad de código escrito por nosotros es menor y mas eficiente gracias a las librerias y los shaders que se usaron. "},{"id":11,"href":"/showcasevc/docs/shortcodes/shaders/Procedural/","title":"Procedural","section":"Shaders","content":" Procedural Texturing # En esta sección aplicamos el ejemplo visto en clase desde el cual se generaba una textura proceduralmente aplicando el patrón truchet tiles, pero en este caso, sobre un objeto distinto.\nDesarrollo del Ejercicio # Implementación # sketch.js # Expand ↕ let pg; let truchetShader; let frames=0; let frames2=100; function preload() { // shader adapted from here: https://thebookofshaders.com/09/ truchetShader = readShader('/showcasevc/p5files/nicholsonSketch/procedural/truchet.frag', { matrices: Tree.NONE, varyings: Tree.NONE }); } function setup() { createCanvas(400, 400, WEBGL); // create frame buffer object to render the procedural texture pg = createGraphics(400, 400, WEBGL); textureMode(NORMAL); noStroke(); pg.noStroke(); pg.textureMode(NORMAL); // use truchetShader to render onto pg pg.shader(truchetShader); // emitResolution, see: // https://github.com/VisualComputing/p5.treegl#macros pg.emitResolution(truchetShader); // https://p5js.org/reference/#/p5.Shader/setUniform truchetShader.setUniform('u_zoom', 3); // pg clip-space quad (i.e., both x and y vertex coordinates ∈ [-1..1]) pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); // set pg as texture texture(pg); } function draw() { background(33); orbitControl(); rotateZ(frames * 0.005); rotateX(frames * 0.005); rotateY(frames * 0.005); truchetShader.setUniform('u_zoom', int(map(frames2*0.1, 0, width, 1, 30))); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); frames++; frames2 = frames2 + 2; sphere(100,200); } function mouseMoved() { // https://p5js.org/reference/#/p5.Shader/setUniform truchetShader.setUniform('u_zoom', int(map(mouseX, 0, width, 1, 30))); // pg clip-space quad (i.e., both x and y vertex coordinates ∈ [-1..1]) pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); } "},{"id":12,"href":"/showcasevc/docs/shortcodes/shaders/texturing/","title":"Texturing","section":"Shaders","content":" Texturing # En esta sección implementamos otros métodos propuestos para la visualización de la luz o el brillo, haciendo uso del value de HSV y lightness de HSL.\nDesarrollo de los Ejercicios # Controles 🕹️ # Haz click y presiona una de las siguientes teclas:\n⬅️ Resetear ⬇️ HSL ( L ) ⬆️ HSV ( V ) ➡️ Promedio Implementación # Promedio RGB # Expand ↕ if(avg){ return 0.3333 * texel.r + 0.3333 * texel.g + 0.3333 * texel.b + 0.1; } HSV V # Teniendo en cuenta que el value del HSV corresponde al máximo entre los valores del RGB, entonces\nExpand ↕ if(val){ if ( texel.r \u0026gt; texel.g \u0026amp;\u0026amp; texel.r \u0026gt; texel.b){ return texel.r * 1.0 + 0.1; } else if (texel.g \u0026gt; texel.r \u0026amp;\u0026amp; texel.g \u0026gt; texel.b){ return texel.g * 1.0 + 0.1; } else if (texel.b \u0026gt; texel.r \u0026amp;\u0026amp; texel.b \u0026gt; texel.g) { return texel.b * 1.0 + 0.1; } } HSL L # A partir de los valores RGB se puede obtener el lightness a partir de el valor medio de estos, es decir: Expand ↕ if(lum){ float max = 0.0; float min = 0.0; if ( texel.r \u0026gt; texel.g \u0026amp;\u0026amp; texel.r \u0026gt; texel.b){ max = texel.r; } else if (texel.g \u0026gt; texel.r \u0026amp;\u0026amp; texel.g \u0026gt; texel.b){ max = texel.g; } else if (texel.b \u0026gt; texel.r \u0026amp;\u0026amp; texel.b \u0026gt; texel.g) { max = texel.b; } if ( texel.r \u0026lt; texel.g \u0026amp;\u0026amp; texel.r \u0026lt; texel.b){ min = texel.r; } else if (texel.g \u0026lt; texel.r \u0026amp;\u0026amp; texel.g \u0026lt; texel.b){ min = texel.g; } else if (texel.b \u0026lt; texel.r \u0026amp;\u0026amp; texel.b \u0026lt; texel.g) { min = texel.b; } return max * 0.5 + min * 0.5 + 0.1; } "},{"id":13,"href":"/showcasevc/docs/shortcodes/terrain/","title":"Terrain","section":"Shortcodes","content":" Creating a terrain with p5.js # Academic report # Introducción: # En el marco de trabajo presentado por el profesor en clase, que enmarcaba varios tipos de ilusiones, nos llamó la atención el titulado MACH BANDS , el cual se enfocaba en la generación de un terreno con una superficie natural con la aplicación del efecto nombrado.\nPara lograr el objetivo utilizamos la libreria p5.js, la cual utilizaremos posiblemente durante los proyectos del semestre por recomendación del profesor y no valemos de una libreria integrada de p5 llamada Perlin noise para tratar de generar una textura natural para el terreno.\nLiteratura: # p5.js: # Es una biblioteca de JavaScript para la codificación creativa, con un enfoque en la realización de la codificación accesible e inclusiva para artistas, diseñadores, educadores, principiantes y cualquier otra persona. P5.js es gratuito y de código abierto porque la filosofia de los creadores es que el software, y las herramientas para aprenderlo, deben ser accesibles para todos.(p5js.org)\nPerlin noise # Es un generador de secuencias aleatorias que produce una sucesión de números armónicos ordenados de forma más natural en compración con la función estándar aleatoria (). Fue inventado por Ken Perlin en la década de 1980 y se ha utilizado desde aplicaciones gráficas para producer textura de procedimiento, movimiento natural, formas, terrenos, etc. (p5js.org)\nLa mayor diferencia con random() es que perlin noise se define en un espacio N-dimensional infinito donde cada par de coordenadas corresponde a un valor semi-aleatorio fijo.\nMetodos # Terrain1 Terrain # En el primer terreno creado usamos TRIANGLE_STRIP como parametro para la función beginShape() Terrain2 Terrain # En esta iteración se hace uso del efecto de la banda de Mach para representar una ilusion de gradiente en la visualización del terreno. Terrain3 Terrain # Por ultimo utilizamos el parametro TRIANGLE_FAN. Terrain Res #Terrain Con TRIANGLE_STRIP la figura a manejar es mas grande y por ende se utilizan menos recursos. Pero el relleno es mas complicado. Con la ilusion generada por la banda de Mach encontramos un rendimiento aceptable. El efecto de relleno visual es natural exceptuando nuestro enfasis al marcar los bordes. Con TRIANGLE_FAN la figura a manejar es mas pequeña y por ende se utilizan mas recursos. Pero el relleno es mas detallado y natural. "},{"id":14,"href":"/showcasevc/docs/shortcodes/terrain/terrain1/","title":"Terrain1","section":"Terrain","content":" Terrain # En el primer terreno creado usamos TRIANGLE_STRIP como parametro para la función beginShape()\n"},{"id":15,"href":"/showcasevc/docs/shortcodes/terrain/terrain2/","title":"Terrain2","section":"Terrain","content":" Terrain # En esta iteración se hace uso del efecto de la banda de Mach para representar una ilusion de gradiente en la visualización del terreno.\n"},{"id":16,"href":"/showcasevc/docs/shortcodes/terrain/terrain3/","title":"Terrain3","section":"Terrain","content":" Terrain # Por ultimo utilizamos el parametro TRIANGLE_FAN.\n"},{"id":17,"href":"/showcasevc/docs/shortcodes/terrain/terrainRes/","title":"Terrain Res","section":"Terrain","content":"#Terrain\nCon TRIANGLE_STRIP la figura a manejar es mas grande y por ende se utilizan menos recursos.\nPero el relleno es mas complicado.\nCon la ilusion generada por la banda de Mach encontramos un rendimiento aceptable.\nEl efecto de relleno visual es natural exceptuando nuestro enfasis al marcar los bordes.\nCon TRIANGLE_FAN la figura a manejar es mas pequeña y por ende se utilizan mas recursos.\nPero el relleno es mas detallado y natural.\n"},{"id":18,"href":"/showcasevc/docs/shortcodes/Transformations/","title":"Transformations","section":"Shortcodes","content":" Creating a Matrix Transformation based cube with p5.js # Academic report # Introducción: # Durante el desarrollo del workshop, buscamos una manera didáctica de llevar a cabo una representación gráfica de la implementación de transformaciones matriciales, y finalmente se planteó que un Cubo de Rubik puede involucrar varios de estos conceptos, especialmente si se habla de Transformaciones Lineales.\nLiteratura: # Transformaciones Lineales # Como se puede imaginar, hay una cantidad inimaginablemente enorme de transformaciones posibles, la mayoría de las cuales sería bastante complicado pensar en ellas. Afortunadamente, el álgebra lineal suele estar limitada a un tipo especial de transformación que es más fácil de entender: las transformaciones lineales.\nVisualmente, esto significa que toda la cuadrícula de puntos 2d \u0026ldquo;sigue\u0026rdquo; con î y ĵ, por así decirlo. Puedes saber que una transformación es lineal si todas esas líneas de cuadrícula que comenzaron paralelas y espaciadas uniformemente permanecen paralelas y espaciadas uniformemente (¿por qué?). En realidad, es un poco más limitado que eso. Si una transformación es lineal, también debe fijar el origen en su lugar.\nMatriz de Rotación # Rotación 2D # Rotación 3D # p5.js: # Es una biblioteca de JavaScript para la codificación creativa, con un enfoque en la realización de la codificación accesible e inclusiva para artistas, diseñadores, educadores, principiantes y cualquier otra persona. P5.js es gratuito y de código abierto porque la filosofia de los creadores es que el software, y las herramientas para aprenderlo, deben ser accesibles para todos.(p5js.org)\nReferencias # 3 blue 1 Brown - Linear Transformations\nMetodos # Rubikcube Rubik\u0026rsquo;s Cube # "},{"id":19,"href":"/showcasevc/docs/shortcodes/Transformations/rubikcube/","title":"Rubikcube","section":"Transformations","content":" Rubik\u0026rsquo;s Cube # "}]